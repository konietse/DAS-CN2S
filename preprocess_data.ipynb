{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "import obspy\n",
    "from obspy.signal.rotate import rotate_ne_rt\n",
    "from obspy.clients.fdsn import Client as FDSN_Client\n",
    "from obspy.geodetics import base\n",
    "from obspy.taup import TauPyModel\n",
    "\n",
    "import pyproj\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate strain rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auspass = FDSN_Client('http://auspass.edu.au:8080')\n",
    "inv = auspass.get_stations(network=\"2B\", level='response')\n",
    "model = TauPyModel(model=\"iasp91\")\n",
    "\n",
    "locations = {}\n",
    "for station in inv[0]:\n",
    "    locations[station.code] = (station.latitude, station.longitude)\n",
    "\n",
    "station_pairs = []\n",
    "for i, station in enumerate(inv[0]):\n",
    "    if station.code in ['SIS01', 'SIS05']:\n",
    "        continue\n",
    "    for j in range(i+1, len(inv[0])):\n",
    "        d = base.gps2dist_azimuth(inv[0][i].latitude, inv[0][i].longitude, \n",
    "                                  inv[0][j].latitude, inv[0][j].longitude)[0]\n",
    "        if d <= 150:\n",
    "            print('Distance between {} and {}: {:.4f} meters'.format(inv[0][i].code, inv[0][j].code, d))\n",
    "            station_pairs.append((inv[0][i].code, inv[0][j].code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/waveforms/'\n",
    "files = glob.glob(path + '*.mseed')\n",
    "\n",
    "n_train = int(len(files) * 0.8)\n",
    "n_test = len(files) - n_train\n",
    "\n",
    "geodesic = pyproj.Geod(ellps='WGS84')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sr = []\n",
    "for i in tqdm(range(n_train)):\n",
    "    st = obspy.read(files[i])\n",
    "\n",
    "    for j, (station1, station2) in enumerate(station_pairs):\n",
    "        \n",
    "        lat1 = locations[station1][0]\n",
    "        lon1 = locations[station1][1]\n",
    "\n",
    "        idx1 = list(locations.keys()).index(station1)\n",
    "        north1 = st[3*idx1].data[:9000]    # station 1 north component\n",
    "        east1 = st[3*idx1+1].data[:9000]   # station 1 east component\n",
    "\n",
    "        lat2 = locations[station2][0]\n",
    "        lon2 = locations[station2][1]\n",
    "        idx2 = list(locations.keys()).index(station2)\n",
    "        north2 = st[3*idx2].data[:9000]    # station 2 north component\n",
    "        east2 = st[3*idx2+1].data[:9000]   # station 2 east component\n",
    "\n",
    "        # Calculate the back azimuth and distance between the two stations\n",
    "        _, back_azimuth, distance_in_m = geodesic.inv(lon1, lat1, lon2, lat2)\n",
    "\n",
    "        # Rotate the waveforms to the radial component\n",
    "        radial1, _ = rotate_ne_rt(north1, east1, back_azimuth % 360)\n",
    "        radial2, _ = rotate_ne_rt(north2, east2, back_azimuth % 360)\n",
    "\n",
    "        train_sr.append((radial1 - radial2) / distance_in_m)\n",
    "\n",
    "if not os.path.exists('./data/preprocessed/'):\n",
    "     os.makedirs('./data/preprocessed/')\n",
    "np.save('./data/preprocessed/SIS-rotated_train_50Hz.npy', np.stack(train_sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sr = []\n",
    "for i in tqdm(range(n_train, len(files))):\n",
    "    st = obspy.read(files[i])\n",
    "\n",
    "    for j, (station1, station2) in enumerate(station_pairs):\n",
    "        \n",
    "        lat1 = locations[station1][0]\n",
    "        lon1 = locations[station1][1]\n",
    "\n",
    "        idx1 = list(locations.keys()).index(station1)\n",
    "        north1 = st[3*idx1].data[:9000]    # station 1 north component\n",
    "        east1 = st[3*idx1+1].data[:9000]   # station 1 east component\n",
    "\n",
    "        lat2 = locations[station2][0]\n",
    "        lon2 = locations[station2][1]\n",
    "        idx2 = list(locations.keys()).index(station2)\n",
    "        north2 = st[3*idx2].data[:9000]    # station 2 north component\n",
    "        east2 = st[3*idx2+1].data[:9000]   # station 2 east component\n",
    "\n",
    "        # Calculate the back azimuth and distance between the two stations\n",
    "        _, back_azimuth, distance_in_m = geodesic.inv(lon1, lat1, lon2, lat2)\n",
    "\n",
    "        # Rotate the waveforms to the radial component\n",
    "        radial1, _ = rotate_ne_rt(north1, east1, back_azimuth % 360)\n",
    "        radial2, _ = rotate_ne_rt(north2, east2, back_azimuth % 360)\n",
    "\n",
    "        test_sr.append((radial1 - radial2) / distance_in_m)\n",
    "\n",
    "if not os.path.exists('./data/preprocessed/'):\n",
    "     os.makedirs('./data/preprocessed/')\n",
    "np.save('./data/preprocessed/SIS-rotated_test_50Hz.npy', np.stack(test_sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = np.stack(test_sr)\n",
    "\n",
    "plt.figure(figsize=(7, 15))\n",
    "\n",
    "for i in range(len(sr[:100])):\n",
    "    x = sr[i] / sr[i].std()\n",
    "    plt.plot(x + 5*i, c=\"k\", lw=0.5, alpha=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract traffic signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 4    # channel spacing\n",
    "fs = 50   # sampling frequency\n",
    "\n",
    "def speed2slowness(speed):\n",
    "    speed = speed * 1000 / 3600\n",
    "    return 1/speed\n",
    "\n",
    "def slowness2speed(slowness):\n",
    "    speed = 1 / slowness\n",
    "    return speed * 3600 / 1000\n",
    "\n",
    "shift2slowness = lambda shift: shift / (dx * fs) # s/m\n",
    "shift2speed = lambda shift: (60 * 60 * dx * fs) / (shift * 1000) # km/h\n",
    "\n",
    "dir1 = './data/index0185_0205/'\n",
    "dir2 = './data/index0275_0295/'\n",
    "files1 = [f'{m//60:02d}{m%60:02d}01' for m in range(7*60+55, 8*60+30)]\n",
    "files2 = [f'{m//60:02d}{m%60:02d}01' for m in range(9*60+30, 10*60)]\n",
    "\n",
    "prefix = 'south30_50Hz_UTC_20230401_'\n",
    "suffix = '.001.h5'\n",
    "\n",
    "def get_chunk(idx, d, f):\n",
    "    file = d + prefix + f[idx] + suffix\n",
    "    data = (h5py.File(file,'r')['DAS'][:].T)[81:4978]\n",
    "    file = d + prefix + f[idx+1] + suffix\n",
    "    data = np.hstack([data, (h5py.File(file,'r')['DAS'][:].T)[81:4978]])\n",
    "    file = d + prefix + f[idx+2] + suffix\n",
    "    data = np.hstack([data, (h5py.File(file,'r')['DAS'][:].T)[81:4978]])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually selected picks\n",
    "train_picks = [(2,190,3200,-0.034), (3,440,2000,-0.035), (4,870,2000,-0.033), (5,1230,1500,-0.03), (7,2230,2000,-0.039),\n",
    "               (15,3050,7500,0.045), \n",
    "               (21,1320,7500,0.048), (23,750,7000,0.05),\n",
    "               (23,1880,7500,0.057), (25,1285,7500,0.051), (27,740,7500,0.054), (29,220,7000,0.052),\n",
    "              ]\n",
    "test_picks = [(1,625,1500,-0.047), (2,935,1500,-0.04), (3,1290,1500,-0.037), (5,2070,1500,-0.048), \n",
    "              (5,4000,6500,0.042), (10,2440,7000,0.048), (12,1720,7000,0.040), (13,1270,7500,0.040), \n",
    "             ]\n",
    "\n",
    "if not os.path.exists('./data/preprocessed/'):\n",
    "     os.makedirs('./data/preprocessed/')\n",
    "\n",
    "for n, p in enumerate(tqdm(train_picks)):\n",
    "    chunk_idx, i, j, slowness = p\n",
    "    shift = dx * fs * slowness\n",
    "    data = get_chunk(chunk_idx, dir1, files1)\n",
    "\n",
    "    strain_rates = []\n",
    "    for x in range(512):\n",
    "        jj = j - int(x*shift)\n",
    "        sr = data[i+x, jj-30*fs:jj+30*fs]\n",
    "        strain_rates.append(sr)\n",
    "    strain_rates = np.stack(strain_rates)\n",
    "\n",
    "    direction = 'inc' if slowness < 0 else 'dec'\n",
    "    np.save('./data/preprocessed/traffic_train_50Hz_{:02d}_{}.npy'.format(n, direction), strain_rates)\n",
    "\n",
    "for n, p in enumerate(tqdm(test_picks)):\n",
    "    chunk_idx, i, j, slowness = p\n",
    "    shift = dx * fs * slowness\n",
    "    data = get_chunk(chunk_idx, dir1, files1)\n",
    "\n",
    "    strain_rates = []\n",
    "    for x in range(512):\n",
    "        jj = j - int(x*shift)\n",
    "        sr = data[i+x, jj-30*fs:jj+30*fs]\n",
    "        strain_rates.append(sr)\n",
    "    strain_rates = np.stack(strain_rates)\n",
    "\n",
    "    direction = 'inc' if slowness < 0 else 'dec'\n",
    "    np.save('./data/preprocessed/traffic_test_50Hz_{:02d}_{}.npy'.format(n, direction), strain_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(strain_rates / strain_rates.std(axis=-1, keepdims=True), origin='lower',\n",
    "           interpolation='none', cmap='seismic', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract real DAS samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general performance for a large earthquake at similar distance, with and without traffic\n",
    "- 2023p202703, M4.7, heavy traffic, distance = 175 km\n",
    "\n",
    "general performance for small earthquakes at increasing distances (with no traffic)\n",
    "- 2023p215452, M2.7, no traffic, distance = 76 km\n",
    "\n",
    "general performance for very small earthquakes very close to the fibre (less than array aperture - 30 km)\n",
    "- 2023p273854, M1.9, minimal traffic, distance = 10 km\n",
    "\n",
    "general performance for small earthquakes at increasing distances (with some traffic)\n",
    "- 2023p181489, M3.3, some traffic, distance = 92 km\n",
    "- 2023p197461, M3.6, some traffic, distance = 159 km\n",
    "- 2023p215753, M3.1, some traffic, distance = 191 km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "train_picks = ['2023p214660', '2023p300579', '2023p303853', '2023p218553', '2023p326201', '2023p313442', \n",
    "               '2023p265881', '2023p171245', '2023p291534', '2023p293493', '2023p278798', '2023p321451', \n",
    "               '2023p166738', '2023p307984', '2023p273765', '2023p209788', '2023p326200', '2023p299899', \n",
    "               '2023p212693', '2023p303138', '2023p252684', '2023p167136', '2023p202039', '2023p235224', \n",
    "               '2023p272380', '2023p172434', '2023p152354', '2023p316959', '2023p178129']\n",
    "test_picks = ['2023p202703', '2023p215452', '2023p273854', '2023p181489', '2023p197461', '2023p215753']\n",
    "\n",
    "os.makedirs('./data/preprocessed/real_train/', exist_ok=True)\n",
    "os.makedirs('./data/preprocessed/real_test/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pick in train_picks:\n",
    "\n",
    "    data = np.hstack([h5py.File(path + pick + '/' + p)['DAS'][:].T for p in sorted([p for p in os.listdir(path + pick + '/')])])\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, sharex=True, figsize=(12,6))\n",
    "    ax.imshow(data, origin='lower', interpolation='nearest', cmap='seismic', aspect='auto', vmin=-100, vmax=100)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    with h5py.File(path + 'preprocessed/real_train/' + pick + '.h5', 'w') as hf:\n",
    "        hf.create_dataset('DAS', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pick in test_picks:\n",
    "\n",
    "    data = np.hstack([h5py.File(path + pick + '/' + p)['DAS'][:].T for p in sorted([p for p in os.listdir(path + pick + '/')])])\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, sharex=True, figsize=(12,6))\n",
    "    ax.imshow(data, origin='lower', interpolation='nearest', cmap='seismic', aspect='auto', vmin=-100, vmax=100)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    with h5py.File(path + 'preprocessed/real_test/' + pick + '.h5', 'w') as hf:\n",
    "        hf.create_dataset('DAS', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
